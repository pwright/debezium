// Metadata created by nebel
//
// ConvertedFromFile: modules/ROOT/pages/connectors/db2.adoc
// ConversionStatus: raw

[id="deploying-debezium-db2-connectors"]
= Deploying a {prodname} Db2 connector by building a custom Kafka Connect container image from a Dockerfile

To deploy a {prodname} Db2 connector, you must build a custom Kafka Connect container image that contains the {prodname} connector archive, and then push this container image to a container registry.
You then need to create the following custom resources (CRs):

* A `KafkaConnect` CR that defines your Kafka Connect instance.
The `image` property in the CR specifies the name of the container image that you create to run your {prodname} connector.
You apply this CR to the OpenShift instance where link:https://access.redhat.com/products/red-hat-amq#streams[Red Hat {StreamsName}] is deployed.
{StreamsName} offers operators and images that bring Apache Kafka to OpenShift.

* A `KafkaConnector` CR that defines your {prodname} Db2 connector.
Apply this CR to the same OpenShift instance where you applied the `KafkaConnect` CR.

.Prerequisites

* Db2 is running and you completed the steps to {LinkDebeziumUserGuide}#setting-up-db2-to-run-a-debezium-connector[set up Db2 to work with a {prodname} connector].

* {StreamsName} is deployed on OpenShift and is running Apache Kafka and Kafka Connect.
For more information, see link:{LinkDeployStreamsOpenShift}[{NameDeployStreamsOpenShift}].

* Podman or Docker is installed.

* The Kafka Connect server has access to Maven Central to download the required JDBC driver for Db2.
  You can also use a local copy of the driver, or one that is available from a local Maven repository or other HTTP server.
* You have an account and permissions to create and manage containers in the container registry (such as `quay.io` or `docker.io`) to which you plan to add the container that will run your Debezium connector.

.Procedure

. Create the {prodname} Db2 container for Kafka Connect:

.. Create a Dockerfile that uses `{DockerKafkaConnect}` as the base image.
For example, from a terminal window, enter the following command:
+
[source,shell,subs="+attributes,+quotes"]
----
cat <<EOF >debezium-container-for-db2.yaml // <1>
FROM {DockerKafkaConnect}
USER root:root
RUN mkdir -p /opt/kafka/plugins/debezium // <2>
RUN curl -O {red-hat-maven-repository}debezium/debezium-connector-{connector-file}/{debezium-version}-redhat-__<build_number>__/debezium-connector-{connector-file}-{debezium-version}-redhat-__<build_number>__-plugin.zip
RUN curl -O https://repo1.maven.org/maven2/com/ibm/db2/jcc/{db2-version}/jcc-{db2-version}.jar
USER 1001
EOF
----
<1> You can specify any file name that you want.
<2> Specifies the path to your Kafka Connect plug-ins directory. If your Kafka Connect plug-ins directory is in a different location, replace this path with the actual path of your directory.
+
The command creates a Dockerfile with the name `debezium-container-for-db2.yaml` in the current directory.

.. Build the container image from the `debezium-container-for-db2.yaml` Docker file that you created in the previous step.
From the directory that contains the file, open a terminal window and enter one of the following commands:
+
[source,shell,options="nowrap"]
----
podman build -t debezium-container-for-db2:latest .
----
+
[source,shell,options="nowrap"]
----
docker build -t debezium-container-for-db2:latest .
----
The preceding commands build a container image with the name `debezium-container-for-db2`.

.. Push your custom image to a container registry, such as quay.io or an internal container registry.
The container registry must be available to the OpenShift instance where you want to deploy the image.
Enter one of the following commands:
+
[source,shell,subs="+quotes"]
----
podman push _<myregistry.io>_/debezium-container-for-db2:latest
----
+
[source,shell,subs="+quotes"]
----
docker push _<myregistry.io>_/debezium-container-for-db2:latest
----

.. Create a new {prodname} Db2 `KafkaConnect` custom resource (CR).
For example, create a `KafkaConnect` CR with the name `dbz-connect.yaml` that specifies `annotations` and `image` properties as shown in the following example:
+
[source,yaml,subs="+attributes"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
  annotations:
    strimzi.io/use-connector-resources: "true" // <1>
spec:
  #...
  image: debezium-container-for-db2  // <2>
----
<1>  `metadata.annotations` indicates to the Cluster Operator that `KafkaConnector` resources are used to configure connectors in this Kafka Connect cluster.
<2>  `spec.image` specifies the name of the image that you created to run your Debezium connector.
This property overrides the `STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE` variable in the Cluster Operator.

.. Apply the `KafkaConnect` CR to the OpenShift Kafka Connect environment by entering the following command:
+
[source,shell,options="nowrap"]
----
oc create -f dbz-connect.yaml
----
+
The command adds a Kafka Connect instance that specifies the name of the image that you created to run your {prodname} connector.

. Create a `KafkaConnector` custom resource that configures your {prodname} Db2 connector instance.
+
You configure a {prodname} Db2 connector in a `.yaml` file that specifies the configuration properties for the connector.
The connector configuration might instruct {prodname} to produce events for a subset of the schemas and tables, or it might set properties so that {prodname} ignores, masks, or truncates values in specified columns that are sensitive, too large, or not needed.
+
The following example configures a {prodname} connector that connects to a Db2 server host, `192.168.99.100`, on port `50000`.
This host has a database named `mydatabase`, a table with the name `inventory`, and `fulfillment` is the server's logical name.
+
.Db2 `inventory-connector.yaml`
[source,yaml,options="nowrap",subs="+attributes"]
----
apiVersion: {KafkaConnectApiVersion}
  kind: KafkaConnector
  metadata:
    name: inventory-connector  // <1>
    labels:
      strimzi.io/cluster: my-connect-cluster
    annotations:
      strimzi.io/use-connector-resources: 'true'
  spec:
    class: io.debezium.connector.db2.Db2Connector // <2>
    tasksMax: 1  // <3>
    config:  // <4>
      database.hostname: 192.168.99.100   // <5>
      database.port: 50000 // <6>
      database.user: db2inst1 // <7>
      database.password: Password! // <8>
      database.dbname: mydatabase // <9>
      database.server.name: fullfillment   // <10>
      database.include.list: public.inventory   // <11>
----
+
.Descriptions of connector configuration settings
[cols="1,7",options="header",subs="+attributes"]
|===
|Item |Description

|1
|The name of the connector when we register it with a Kafka Connect cluster.

|2
|The name of this Db2 connector class.

|3
|Only one task should operate at any one time.

|4
|The connectorâ€™s configuration.

|5
|The database host, which is the address of the Db2 instance.

|6
|The port number of the Db2 instance.

|7
|The name of the Db2 user.

|8
|The password for the Db2 user.

|9
|The name of the database to capture changes from.

|10
|The logical name of the Db2 instance/cluster, which forms a namespace and is used in the names of the Kafka topics to which the connector writes, the names of Kafka Connect schemas, and the namespaces of the corresponding Avro schema when the xref:{link-avro-serialization}#avro-serialization[Avro Connector] is used.

|11
|A list of all tables whose changes {prodname} should capture.

|===

. Create your connector instance with Kafka Connect.
For example, if you saved your `KafkaConnector` resource in the `inventory-connector.yaml` file, you would run the following command:
+
[source,shell,options="nowrap"]
----
oc apply -f inventory-connector.yaml
----
+
The preceding command registers `inventory-connector` and the connector starts to run against the `mydatabase` database as defined in the `KafkaConnector` CR.




For the complete list of the configuration properties that you can set for the {prodname} Db2 connector, see xref:{link-db2-connector}#db2-connector-properties[Db2 connector properties].


.Results

After the connector starts, it xref:{link-db2-connector}#db2-snapshots[performs a consistent snapshot] of the Db2 database tables that the connector is configured to capture changes for.
The connector then starts generating data change events for row-level operations and streaming change event records to Kafka topics.

