// Metadata created by nebel
//
// ConvertedFromTitle: Connector properties
// ConvertedFromFile: modules/ROOT/pages/connectors/db2.adoc
// ConversionStatus: raw
// ConvertedFromID: db2-connector-properties

[id="descriptions-of-debezium-db2-connector-configuration-properties"]
= Description of {prodname} Db2 connector configuration properties

The {prodname} Db2 connector has numerous configuration properties that you can use to achieve the right connector behavior for your application.
Many properties have default values.
Information about the properties is organized as follows:

* xref:db2-required-configuration-properties[Required configuration properties]
* xref:db2-advanced-configuration-properties[Advanced configuration properties]
* xref:debezium-db2-connector-database-history-configuration-properties[Database history connector configuration properties] that control how {prodname} processes events that it reads from the database history topic.
** xref:debezium-db2-connector-pass-through-database-driver-configuration-properties[Pass-through database history properties]
* xref:debezium-db2-connector-pass-through-database-driver-configuration-properties[Pass-through database driver properties] that control the behavior of the database driver.


[id="db2-required-configuration-properties"]
.Required {prodname} Db2 connector configuration properties

The following configuration properties are _required_ unless a default value is available.

[cols="30%a,25%a,45%a",options="header"]
|===
|Property |Default |Description

|[[db2-property-name]]<<db2-property-name, `+name+`>>
|No default
|Unique name for the connector. Attempting to register again with the same name will fail. This property is required by all Kafka Connect connectors.

|[[db2-property-connector-class]]<<db2-property-connector-class, `+connector.class+`>>
|No default
|The name of the Java class for the connector. Always use a value of `io.debezium.connector.db2.Db2Connector` for the Db2 connector.

|[[db2-property-tasks-max]]<<db2-property-tasks-max, `+tasks.max+`>>
|`1`
|The maximum number of tasks that should be created for this connector. The Db2 connector always uses a single task and therefore does not use this value, so the default is always acceptable.

|[[db2-property-database-hostname]]<<db2-property-database-hostname, `+database.hostname+`>>
|No default
|IP address or hostname of the Db2 database server.

|[[db2-property-database-port]]<<db2-property-database-port, `+database.port+`>>
|`50000`
|Integer port number of the Db2 database server.

|[[db2-property-database-user]]<<db2-property-database-user, `+database.user+`>>
|No default
|Name of the Db2 database user for connecting to the Db2 database server.

|[[db2-property-database-password]]<<db2-property-database-password, `+database.password+`>>
|No default
|Password to use when connecting to the Db2 database server.

|[[db2-property-database-dbname]]<<db2-property-database-dbname, `+database.dbname+`>>
|No default
|The name of the Db2 database from which to stream the changes

|[[db2-property-database-server-name]]<<db2-property-database-server-name, `+database.server.name+`>>
|No default
|Logical name that identifies and provides a namespace for the particular Db2 database server that hosts the database for which {prodname} is capturing changes.
Only alphanumeric characters, hyphens, dots and underscores must be used in the database server logical name.
The logical name should be unique across all other connectors, since it is used as a topic name prefix for all Kafka topics that receive records from this connector.
+
[WARNING]
====
Do not change the value of this property.
If you change the name value, after a restart, instead of continuing to emit events to the original topics, the connector emits subsequent events to topics whose names are based on the new value.
The connector is also unable to recover its database history topic.
====

|[[db2-property-table-include-list]]<<db2-property-table-include-list, `+table.include.list+`>>
|No default
|An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you want the connector to capture. Any table not included in the include list does not have its changes captured. Each identifier is of the form _schemaName_._tableName_. By default, the connector captures changes in every non-system table. Do not also set the `table.exclude.list` property.

|[[db2-property-table-exclude-list]]<<db2-property-table-exclude-list, `+table.exclude.list+`>>
|No default
|An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you do not want the connector to capture. The connector captures changes in each non-system table that is not included in the exclude list. Each identifier is of the form _schemaName_._tableName_. Do not also set the `table.include.list` property.

|[[db2-property-column-exclude-list]]<<db2-property-column-exclude-list, `+column.exclude.list+`>>
|_empty string_
|An optional, comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event values.
Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_.
Primary key columns are always included in the event's key, even if they are excluded from the value.

|[[db2-property-column-mask-hash]]<<db2-property-column-mask-hash, `column.mask.hash._hashAlgorithm_.with.salt._salt_`>>
|_n/a_
|An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns.
Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_.
In the resulting change event record, the values for the specified columns are replaced with pseudonyms. +

A pseudonym consists of the hashed value that results from applying the specified _hashAlgorithm_ and _salt_.
Based on the hash function that is used, referential integrity is maintained, while column values are replaced with pseudonyms.
Supported hash functions are described in the {link-java7-standard-names}[MessageDigest section] of the Java Cryptography Architecture Standard Algorithm Name Documentation. +
 +
In the following example, `CzQMA0cB5K` is a randomly selected salt. +

----
column.mask.hash.SHA-256.with.salt.CzQMA0cB5K = inventory.orders.customerName, inventory.shipment.customerName
----

If necessary, the pseudonym is automatically shortened to the length of the column.
The connector configuration can include multiple properties that specify different hash algorithms and salts. +
 +
Depending on the _hashAlgorithm_ used, the _salt_ selected, and the actual data set, the resulting data set might not be completely masked.

|[[db2-property-time-precision-mode]]<<db2-property-time-precision-mode, `+time.precision.mode+`>>
|`adaptive`
| Time, date, and timestamps can be represented with different kinds of precision: +
 +
`adaptive` captures the time and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column's type. +
 +
`connect` always represents time and timestamp values by using Kafka Connect's built-in representations for `Time`, `Date`, and `Timestamp`, which uses millisecond precision regardless of the database columns' precision. See xref:{link-db2-connector}#db2-temporal-values[temporal values].

|[[db2-property-tombstones-on-delete]]<<db2-property-tombstones-on-delete, `+tombstones.on.delete+`>>
|`true`
|Controls whether a _delete_ event is followed by a tombstone event. +
 +
`true` - a delete operation is represented by a _delete_ event and a subsequent tombstone event.  +
 +
`false` - only a _delete_ event is emitted. +
 +
After a source record is deleted, emitting a tombstone event (the default behavior) allows Kafka to completely delete all events that pertain to the key of the deleted row in case {link-kafka-docs}/#compaction[log compaction] is enabled for the topic.

|[[db2-property-include-schema-changes]]<<db2-property-include-schema-changes, `+include.schema.changes+`>>
|`true`
|Boolean value that specifies whether the connector should publish changes in the database schema to a Kafka topic with the same name as the database server ID. Each schema change is recorded with a key that contains the database name and a value that is a JSON structure that describes the schema update. This is independent of how the connector internally records database history.

|[[db2-property-column-truncate-to-length-chars]]<<db2-property-column-truncate-to-length-chars, `+column.truncate.to._length_.chars+`>>
|_n/a_
|An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns. Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_. In change event records, values in these columns are truncated if they are longer than the number of characters specified by _length_ in the property name. You can specify multiple properties with different lengths in a single configuration. Length must be a positive integer, for example, `column.truncate.to.20.chars`.

|[[db2-property-column-mask-with-length-chars]]<<db2-property-column-mask-with-length-chars, `+column.mask.with._length_.chars+`>>
|_n/a_
|An optional, comma-separated list of regular expressions that match the fully-qualified names of character-based columns. Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_. In change event values, the values in the specified table columns are replaced with _length_ number of asterisk (`*`) characters. You can specify multiple properties with different lengths in a single configuration. Length must be a positive integer or zero. When you specify zero, the connector replaces a value with an empty string.

|[[db2-property-column-propagate-source-type]]<<db2-property-column-propagate-source-type, `+column.propagate.source.type+`>>
|_n/a_
|An optional, comma-separated list of regular expressions that match the fully-qualified names of columns.
Fully-qualified names for columns are of the form _databaseName_._tableName_._columnName_, or _databaseName_._schemaName_._tableName_._columnName_. +
 +
For each specified column, the connector adds the column's original type and original length as parameters to the corresponding field schemas in the emitted change records.
Add the following schema parameters to propagate the original type name and the original length for variable-width types: +
 +
`pass:[_]pass:[_]debezium.source.column.type` +
`pass:[_]pass:[_]debezium.source.column.length` +
`pass:[_]pass:[_]debezium.source.column.scale` +
 +
This property is useful for properly sizing corresponding columns in sink databases.

|[[db2-property-datatype-propagate-source-type]]<<db2-property-datatype-propagate-source-type, `+datatype.propagate.source.type+`>>
|_n/a_
|An optional, comma-separated list of regular expressions that match the database-specific data type name for some columns. Fully-qualified data type names are of the form _databaseName_._tableName_._typeName_, or _databaseName_._schemaName_._tableName_._typeName_. +
 +
For these data types, the connector adds parameters to the corresponding field schemas in emitted change records. The added parameters specify the original type and length of the column: +
 +
`pass:[_]pass:[_]debezium.source.column.type` +
`pass:[_]pass:[_]debezium.source.column.length` +
`pass:[_]pass:[_]debezium.source.column.scale` +
 +
These parameters propagate a column's original type name and length, for variable-width types, respectively. This property is useful for properly sizing corresponding columns in sink databases. +
 +
See xref:{link-db2-connector}#db2-data-types[Db2 data types] for the list of Db2-specific data type names.

|[[db2-property-message-key-columns]]<<db2-property-message-key-columns, `+message.key.columns+`>>
|_empty string_
|A list of expressions that specify the columns that the connector uses to form custom message keys for change event records that it publishes to the Kafka topics for specified tables.

By default, {prodname} uses the primary key column of a table as the message key for records that it emits.
In place of the default, or to specify a key for tables that lack a primary key, you can configure custom message keys based on one or more columns. +
 +
To establish a custom message key for a table, list the table, followed by the columns to use as the message key.
Each list entry takes the following format: +
 +
`_<fully-qualified_tableName>_:_<keyColumn>_,_<keyColumn>_` +
 +
To base a table key on multiple column names, insert commas between the column names. +
Each fully-qualified table name is a regular expression in the following format: +

`_<schemaName>_._<tableName>_` +

The property can list entries for multiple tables.
Use a semicolon to separate entries for different tables in the list. +
 +
The following example sets the message key for the tables `inventory.customers` and `purchaseorders`: +
 +
`inventory.customers:pk1,pk2;(.*).purchaseorders:pk3,pk4` +
 +
In the preceding example, the columns `pk1` and `pk2` are specified as the message key for the table `inventory.customer`.
For `purchaseorders` tables in any schema, the columns `pk3` and `pk4` serve as the message key.

|[[db2-property-schema-name-adjustment-mode]]<<db2-property-schema-name-adjustment-mode,`+schema.name.adjustment.mode+`>>
|avro
|Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Possible settings:  +

* `avro` replaces the characters that cannot be used in the Avro type name with underscore. +
* `none` does not apply any adjustment. +

|===

[id="db2-advanced-configuration-properties"]
.Advanced connector configuration properties

The following _advanced_ configuration properties have defaults that work in most situations and therefore rarely need to be specified in the connector's configuration.

[cols="30%a,25%a,45%a",options="header"]
|===
|Property |Default |Description

|[[db2-property-converters]]<<db2-property-converters, `converters`>>
|No default
|Enumerates a comma-separated list of the symbolic names of the {link-prefix}:{link-custom-converters}#custom-converters[custom converter] instances that the connector can use.
For example, +

`isbn`

You must set the `converters` property to enable the connector to use a custom converter.

For each converter that you configure for a connector, you must also add a `.type` property, which specifies the fully-qualifed name of the class that implements the converter interface.
The `.type` property uses the following format: +

`_<converterSymbolicName>_.type` +

For example, +

 isbn.type: io.debezium.test.IsbnConverter

If you want to further control the behavior of a configured converter, you can add one or more configuration parameters to pass values to the converter.
To associate any additional configuration parameter with a converter, prefix the parameter names with the symbolic name of the converter. +
For example, +

 isbn.schema.name: io.debezium.db2.type.Isbn

|[[db2-property-snapshot-mode]]<<db2-property-snapshot-mode, `+snapshot.mode+`>>
|`initial`
|Specifies the criteria for performing a snapshot when the connector starts: +
 +
`initial` - For tables in capture mode, the connector takes a snapshot of the schema for the table and the data in the table. This is useful for populating Kafka topics with a complete representation of the data. +
 +
`schema_only` - For tables in capture mode, the connector takes a snapshot of only the schema for the table. This is useful when only the changes that are happening from now on need to be emitted to Kafka topics. After the snapshot is complete, the connector continues by reading change events from the database's redo logs.

|[[db2-property-snapshot-isolation-mode]]<<db2-property-snapshot-isolation-mode, `+snapshot.isolation.mode+`>>
|`repeatable_read`
|During a snapshot, controls the transaction isolation level and how long the connector locks the tables that are in capture mode. The possible values are: +
 +
`read_uncommitted` - Does not prevent other transactions from updating table rows during an initial snapshot. This mode has no data consistency guarantees; some data might be lost or corrupted. +
 +
`read_committed` - Does not prevent other transactions from updating table rows during an initial snapshot. It is possible for a new record to appear twice: once in the initial snapshot and once in the streaming phase. However, this consistency level is appropriate for data mirroring. +
 +
`repeatable_read` - Prevents other transactions from updating table rows during an initial snapshot. It is possible for a new record to appear twice: once in the initial snapshot and once in the streaming phase. However, this consistency level is appropriate for data mirroring. +
 +
`exclusive` - Uses repeatable read isolation level but takes an  exclusive lock for all tables to be read. This mode prevents other transactions from updating table rows during an initial snapshot. Only `exclusive` mode guarantees full consistency; the initial snapshot and streaming logs constitute a linear history.

|[[db2-property-event-processing-failure-handling-mode]]<<db2-property-event-processing-failure-handling-mode, `+event.processing.failure.handling.mode+`>>
|`fail`
|Specifies how the connector handles exceptions during processing of events. The possible values are: +
 +
`fail` - The connector logs the offset of the problematic event and stops processing. +
 +
`warn` - The connector logs the offset of the problematic event and continues processing with the next event. +
 +
`skip` - The connector skips the problematic event and continues processing with the next event.

|[[db2-property-poll-interval-ms]]<<db2-property-poll-interval-ms, `+poll.interval.ms+`>>
|`1000`
|Positive integer value that specifies the number of milliseconds the connector should wait for new change events to appear before it starts processing a batch of events. Defaults to 1000 milliseconds, or 1 second.

|[[db2-property-max-batch-size]]<<db2-property-max-batch-size, `+max.batch.size+`>>
|`2048`
|Positive integer value that specifies the maximum size of each batch of events that the connector processes.

|[[db2-property-max-queue-size]]<<db2-property-max-queue-size, `+max.queue.size+`>>
|`8192`
|Positive integer value that specifies the maximum number of records that the blocking queue can hold.
When {prodname} reads events streamed from the database, it places the events in the blocking queue before it writes them to Kafka.
The blocking queue can provide backpressure for reading change events from the database
in cases where the connector ingests messages faster than it can write them to Kafka, or when Kafka becomes unavailable.
Events that are held in the queue are disregarded when the connector periodically records offsets.
Always set the value of `max.queue.size` to be larger than the value of xref:{context}-property-max-batch-size[`max.batch.size`].

|[[db2-property-max-queue-size-in-bytes]]<<db2-property-max-queue-size-in-bytes, `+max.queue.size.in.bytes+`>>
|`0`
|A long integer value that specifies the maximum volume of the blocking queue in bytes.
By default, volume limits are not specified for the blocking queue.
To specify the number of bytes that the queue can consume, set this property to a positive long value. +
If xref:db2-property-max-queue-size[`max.queue.size`] is also set, writing to the queue is blocked when the size of the queue reaches the limit specified by either property.
For example, if you set `max.queue.size=1000`, and `max.queue.size.in.bytes=5000`, writing to the queue is blocked after the queue contains 1000 records, or after the volume of the records in the queue reaches 5000 bytes.

|[[db2-property-heartbeat-interval-ms]]<<db2-property-heartbeat-interval-ms, `+heartbeat.interval.ms+`>>
|`0`
|Controls how frequently the connector sends heartbeat messages to a Kafka topic. The default behavior is that the connector does not send heartbeat messages. +
 +
Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages. +
 +
Heartbeat messages are useful when there are many updates in a database that is being tracked but only a tiny number of updates are in tables that are in capture mode. In this situation, the connector reads from the database transaction log as usual but rarely emits change records to Kafka. This means that the connector has few opportunities to send the latest offset to Kafka. Sending heartbeat messages enables the connector to send the latest offset to Kafka.

|[[db2-property-heartbeat-topics-prefix]]<<db2-property-heartbeat-topics-prefix, `+heartbeat.topics.prefix+`>>
|`__debezium-heartbeat`
|Specifies the prefix for the name of the topic to which the connector sends heartbeat messages. The format for this topic name is  `<heartbeat.topics.prefix>.<server.name>`.

|[[db2-property-snapshot-delay-ms]]<<db2-property-snapshot-delay-ms, `+snapshot.delay.ms+`>>
|No default
|An interval in milliseconds that the connector should wait before performing a snapshot when the connector starts. If you are starting multiple connectors in a cluster, this property is useful for avoiding snapshot interruptions, which might cause re-balancing of connectors.

|[[db2-property-snapshot-fetch-size]]<<db2-property-snapshot-fetch-size, `+snapshot.fetch.size+`>>
|`2000`
|During a snapshot, the connector reads table content in batches of rows. This property specifies the maximum number of rows in a batch.

|[[db2-property-snapshot-lock-timeout-ms]]<<db2-property-snapshot-lock-timeout-ms, `+snapshot.lock.timeout.ms+`>>
|`10000`
|Positive integer value that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot. If the connector cannot acquire table locks in this interval, the snapshot fails. xref:{link-db2-connector}#db2-snapshots[How the connector performs snapshots] provides details. Other possible settings are: +
 +
`0` -  The connector immediately fails when it cannot obtain a lock. +
 +
`-1` - The connector waits infinitely.

|[[db2-property-snapshot-select-statement-overrides]]<<db2-property-snapshot-select-statement-overrides, `+snapshot.select.statement.overrides+`>>
|No default
|Specifies the table rows to include in a snapshot.
Use the property if you want a snapshot to include only a subset of the rows in a table.
This property affects snapshots only.
It does not apply to events that the connector reads from the log.

The property contains a comma-separated list of fully-qualified table names in the form `_<schemaName>.<tableName>_`. For example, +
 +
`+"snapshot.select.statement.overrides": "inventory.products,customers.orders"+` +
 +
For each table in the list, add a further configuration property that specifies the `SELECT` statement for the connector to run on the table when it takes a snapshot.
The specified `SELECT` statement determines the subset of table rows to include in the snapshot.
Use the following format to specify the name of this `SELECT` statement property: +
 +
`snapshot.select.statement.overrides._<schemaName>_._<tableName>_`.
For example,
`snapshot.select.statement.overrides.customers.orders`. +
 +
Example:

From a `customers.orders` table that includes the soft-delete column, `delete_flag`, add the following properties if you want a snapshot to include only those records that are not soft-deleted:

----
"snapshot.select.statement.overrides": "customer.orders",
"snapshot.select.statement.overrides.customer.orders": "SELECT * FROM [customers].[orders] WHERE delete_flag = 0 ORDER BY id DESC"
----

In the resulting snapshot, the connector includes only the records for which `delete_flag = 0`.
|[[db2-property-sanitize-field-names]]<<db2-property-sanitize-field-names, `+sanitize.field.names+`>>
|`true` if connector configuration sets the `key.converter` or `value.converter` property to the Avro converter.

`false` if not.
|Indicates whether field names are sanitized to adhere to xref:{link-avro-serialization}#avro-naming[Avro naming requirements].

|[[db2-property-provide-transaction-metadata]]<<db2-property-provide-transaction-metadata, `+provide.transaction.metadata+`>>
|`false`
|Determines whether the connector generates events with transaction boundaries and enriches change event envelopes with transaction metadata. Specify `true` if you want the connector to do this. See xref:{link-db2-connector}#db2-transaction-metadata[Transaction metadata] for details.

|[[db2-property-transaction-topic]]<<db2-property-transaction-topic, `transaction.topic`>>
|`${database.server.name}.transaction`
|Controls the name of the topic to which the connector sends transaction metadata messages. The placeholder `${database.server.name}` can be used for referring to the connector's logical name; defaults to `${database.server.name}.transaction`, for example `dbserver1.transaction`.

|[[db2-property-skipped-operations]]<<db2-property-skipped-operations, `+skipped.operations+`>>
|No default
| comma-separated list of operation types that will be skipped during streaming.
The operations include: `c` for inserts/create, `u` for updates, and `d` for deletes.
By default, no operations are skipped.

|[[db2-property-signal-data-collection]]<<db2-property-signal-data-collection, `+signal.data.collection+`>>
|No default
| Fully-qualified name of the data collection that is used to send xref:{link-signalling}#debezium-signaling-enabling-signaling[signals] to the connector.
Use the following format to specify the collection name: +
`_<schemaName>_._<tableName>_`

|[[db2-property-incremental-snapshot-chunk-size]]<<db2-property-incremental-snapshot-chunk-size, `+incremental.snapshot.chunk.size+`>>
|`1024`
|The maximum number of rows that the connector fetches and reads into memory during an incremental snapshot chunk.
Increasing the chunk size provides greater efficiency, because the snapshot runs fewer snapshot queries of a greater size.
However, larger chunk sizes also require more memory to buffer the snapshot data.
Adjust the chunk size to a value that provides the best performance in your environment.

|===

[id="debezium-db2-connector-database-history-configuration-properties"]
.{prodname} connector database history configuration properties

{prodname} provides a set of `database.history.*` properties that control how the connector interacts with the schema history topic.

The following table describes the `database.history` properties for configuring the {prodname} connector.

.Connector database history configuration properties
[cols="33%a,17%a,50%a",options="header",subs="+attributes"]
|===
|Property |Default |Description
|[[{context}-property-database-history-kafka-topic]]<<{context}-property-database-history-kafka-topic, `+database.history.kafka.topic+`>>
|
|The full name of the Kafka topic where the connector stores the database schema history.

|[[{context}-property-database-history-kafka-bootstrap-servers]]<<{context}-property-database-history-kafka-bootstrap-servers, `+database.history.kafka.bootstrap.servers+`>>
|
|A list of host/port pairs that the connector uses for establishing an initial connection to the Kafka cluster. This connection is used for retrieving the database schema history previously stored by the connector, and for writing each DDL statement read from the source database. Each pair should point to the same Kafka cluster used by the Kafka Connect process.

|[[{context}-property-database-history-kafka-recovery-poll-interval-ms]]<<{context}-property-database-history-kafka-recovery-poll-interval-ms, `+database.history.kafka.recovery.poll.interval.ms+`>>
|`100`
|An integer value that specifies the maximum number of milliseconds the connector should wait during startup/recovery while polling for persisted data. The default is 100ms.

|[[{context}-property-database-history-kafka-query-timeout-ms]]<<{context}-property-database-history-kafka-query-timeout-ms, `+database.history.kafka.query.timeout.ms+`>>
|`3000`
|An integer value that specifies the maximum number of milliseconds the connector should wait while fetching cluster information using Kafka admin client.

|[[{context}-property-database-history-kafka-recovery-attempts]]<<{context}-property-database-history-kafka-recovery-attempts, `+database.history.kafka.recovery.attempts+`>>
|`4`
|The maximum number of times that the connector should try to read persisted history data before the connector recovery fails with an error. The maximum amount of time to wait after receiving no data is `recovery.attempts` x `recovery.poll.interval.ms`.

|[[{context}-property-database-history-skip-unparseable-ddl]]<<{context}-property-database-history-skip-unparseable-ddl, `+database.history.skip.unparseable.ddl+`>>
|`false`
|A Boolean value that specifies whether the connector should ignore malformed or unknown database statements or stop processing so a human can fix the issue.
The safe default is `false`.
Skipping should be used only with care as it can lead to data loss or mangling when the binlog is being processed.

|[[{context}-property-database-history-store-only-captured-tables-ddl]]<<{context}-property-database-history-store-only-captured-tables-ddl, `+database.history.store.only.captured.tables.ddl+`>>
|`false`
|A Boolean value that specifies whether the connector should record all DDL statements +

`true` records only those DDL statements that are relevant to tables whose changes are being captured by {prodname}. Set to `true` with care because missing data might become necessary if you change which tables have their changes captured. +

The safe default is `false`.
|===

[id="{context}-pass-through-database-history-properties-for-configuring-producer-and-consumer-clients"]
.Pass-through database history properties for configuring producer and consumer clients
{empty} +
{prodname} relies on a Kafka producer to write schema changes to database history topics.
Similarly, it relies on a Kafka consumer to read from database history topics when a connector starts.
You define the configuration for the Kafka producer and consumer clients by assigning values to a set of pass-through configuration properties that begin with the `database.history.producer.\*` and `database.history.consumer.*` prefixes.
The pass-through producer and consumer database history properties control a range of behaviors, such as how these clients secure connections with the Kafka broker, as shown in the following example:

[source,indent=0]
----
database.history.producer.security.protocol=SSL
database.history.producer.ssl.keystore.location=/var/private/ssl/kafka.server.keystore.jks
database.history.producer.ssl.keystore.password=test1234
database.history.producer.ssl.truststore.location=/var/private/ssl/kafka.server.truststore.jks
database.history.producer.ssl.truststore.password=test1234
database.history.producer.ssl.key.password=test1234

database.history.consumer.security.protocol=SSL
database.history.consumer.ssl.keystore.location=/var/private/ssl/kafka.server.keystore.jks
database.history.consumer.ssl.keystore.password=test1234
database.history.consumer.ssl.truststore.location=/var/private/ssl/kafka.server.truststore.jks
database.history.consumer.ssl.truststore.password=test1234
database.history.consumer.ssl.key.password=test1234
----

{prodname} strips the prefix from the property name before it passes the property to the Kafka client.

See the Kafka documentation for more details about link:https://kafka.apache.org/documentation.html#producerconfigs[Kafka producer configuration properties] and link:https://kafka.apache.org/documentation.html#consumerconfigs[Kafka consumer configuration properties].

[id="debezium-db2-connector-pass-through-database-driver-configuration-properties"]
.{prodname} connector pass-through database driver configuration properties

The {prodname} connector provides for pass-through configuration of the database driver.
Pass-through database properties begin with the prefix `database.*`.
For example, the connector passes properties such as `database.foobar=false` to the JDBC URL.

As is the case with the xref:{context}-pass-through-database-history-properties-for-configuring-producer-and-consumer-clients[pass-through properties for database history clients], {prodname} strips the prefixes from the properties before it passes them to the database driver.

