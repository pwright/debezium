// Metadata created by nebel
//
// ConvertedFromTitle: Data type mappings
// ConvertedFromFile: modules/ROOT/pages/connectors/oracle.adoc
// ConversionStatus: raw
// ConvertedFromID: oracle-data-type-mappings

[id="how-debezium-oracle-connectors-map-data-types"]
= How {prodname} Oracle connectors map data types

When the {prodname} {connector-name} connector detects a change in the value of a table row, it emits a change event that represents the change.
Each change event record is structured in the same way as the original table, with the event record containing a field for each column value.
The data type of a table column determines how the connector represents the column's values in change event fields, as shown in the tables in the following sections.

For each column in a table, {prodname} maps the source data type to a _literal type_ and, and in some cases, a _semantic type_, in the corresponding event field.

Literal types:: Describe how the value is literally represented, using one of the following Kafka Connect schema types: `INT8`, `INT16`, `INT32`, `INT64`, `FLOAT32`, `FLOAT64`, `BOOLEAN`, `STRING`, `BYTES`, `ARRAY`, `MAP`, and `STRUCT`.

Semantic types:: Describe how the Kafka Connect schema captures the _meaning_ of the field, by using the name of the Kafka Connect schema for the field.

If the default data type conversions do not meet your needs, you can {link-prefix}:{link-custom-converters}#custom-converters[create a custom converter] for the connector.

For some Oracle large object (CLOB, NCLOB, and BLOB) and numeric data types, you can manipulate the way that the connector performs the type mapping by changing default configuration property settings.
For more information about how {prodname} properties control mappings for these data types, see xref:oracle-binary-character-lob-types[Binary and Character LOB types] and xref:oracle-numeric-types[Numeric types].

For more information about how the {prodname} connector maps Oracle data types, see the following topics:

* xref:oracle-character-types[]
* xref:oracle-binary-character-lob-types[]
* xref:oracle-numeric-types[]
* xref:oracle-boolean-types[]
* xref:oracle-temporal-types[]
* xref:oracle-rowid-types[]
* xref:oracle-user-defined-types[]
* xref:oracle-supplied-types[]
* xref:oracle-default-values[]



[id="oracle-character-types"]
.Character types

The following table describes how the connector maps basic character types.

.Mappings for Oracle basic character types
[cols="20%a,15%a,55%a",options="header"]
|===
|Oracle Data Type
|Literal type (schema type)
|Semantic type (schema name) and Notes

|`CHAR[(M)]`
|`STRING`
|n/a

|`NCHAR[(M)]`
|`STRING`
|n/a

|`NVARCHAR2[(M)]`
|`STRING`
|n/a

|`VARCHAR[(M)]`
|`STRING`
|n/a

|`VARCHAR2[(M)]`
|`STRING`
|n/a

|===

[id="oracle-binary-character-lob-types"]
.Binary and Character LOB types
[IMPORTANT]
====
Use of the `BLOB`, `CLOB`, and `NCLOB` with the {prodname} Oracle connector is a Technology Preview feature only.
Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete.
Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview[https://access.redhat.com/support/offerings/techpreview].
====
The following table describes how the connector maps binary and character large object (LOB) data types.

.Mappings for Oracle binary and character LOB types
[cols="20%a,15%a,55%a",options="header"]
|===
|Oracle Data Type
|Literal type (schema type)
|Semantic type (schema name) and Notes

|`BFILE`
|n/a
|_This data type is not supported_

|`BLOB`
|`BYTES`
|Either the raw bytes (the default), a base64-encoded String, or a hex-encoded String, based on the xref:oracle-property-binary-handling-mode[`binary.handling.mode`] connector configuration property setting.

|`CLOB`
|`STRING`
|n/a

|`LONG`
|n/a
|_This data type is not supported._

|`LONG RAW`
|n/a
|_This data type is not supported._

|`NCLOB`
|`STRING`
|n/a

|`RAW`
|n/a
|_This data type is not supported._

|===

[NOTE]
====
Oracle only supplies column values for `CLOB`, `NCLOB`, and `BLOB` data types if they're explicitly set or changed in a SQL statement.
As a result, change events never contain the value of an unchanged `CLOB`, `NCLOB`, or `BLOB` column.
Instead, they contain placeholders as defined by the connector property, `unavailable.value.placeholder`.

If the value of a `CLOB`, `NCLOB`, or `BLOB` column is updated, the new value is placed in the `after` element of the corresponding update change event.
The `before` element contains the unavailable value placeholder.
====

[id="oracle-numeric-types"]
.Numeric types

The following table describes how the {prodname} Oracle connector maps numeric types.

[NOTE]
====
You can modify the way that the connector maps the Oracle `DECIMAL`, `NUMBER`, `NUMERIC`, and `REAL` data types by changing the value of the connector's xref:oracle-property-decimal-handling-mode[`decimal.handling.mode`] configuration property.
When the property is set to its default value of `precise`, the connector maps these Oracle data types to the Kafka Connect `org.apache.kafka.connect.data.Decimal` logical type, as indicated in the table.
When the value of the property is set to `double` or `string`, the connector uses alternate mappings for some Oracle data types.
For more information, see the _Semantic type and Notes_ column in the following table.
====
.Mappings for Oracle numeric data types
[cols="20%a,15%a,55%a",options="header"]
|===
|Oracle Data Type
|Literal type (schema type)
|Semantic type (schema name) and Notes

|`BINARY_FLOAT`
|`FLOAT32`
|n/a

|`BINARY_DOUBLE`
|`FLOAT64`
|n/a

|`DECIMAL[(P, S)]`
|`BYTES` / `INT8` / `INT16` / `INT32` / `INT64`
|`org.apache.kafka.connect.data.Decimal` if using `BYTES` +
 +
Handled equivalently to `NUMBER` (note that S defaults to 0 for `DECIMAL`).

When the `decimal.handling.mode` property is set to `double`, the connector represents `DECIMAL` values as Java `double` values with schema type `FLOAT64`.

When the `decimal.handling.mode` property is set to `string`, the connector represents DECIMAL values as their formatted string representation with schema type `STRING`.

|`DOUBLE PRECISION`
|`STRUCT`
|`io.debezium.data.VariableScaleDecimal` +
 +
Contains a structure with two fields: `scale` of type `INT32` that contains the scale of the transferred value and `value` of type `BYTES` containing the original value in an unscaled form.

|`FLOAT[(P)]`
|`STRUCT`
|`io.debezium.data.VariableScaleDecimal` +
 +
Contains a structure with two fields: `scale` of type `INT32` that contains the scale of the transferred value and `value` of type `BYTES` containing the original value in an unscaled form.

|`INTEGER`, `INT`
|`BYTES`
|`org.apache.kafka.connect.data.Decimal` +
 +
`INTEGER` is mapped in Oracle to NUMBER(38,0) and hence can hold values larger than any of the `INT` types could store

|`NUMBER[(P[, *])]`
|`STRUCT`
|`io.debezium.data.VariableScaleDecimal` +
 +
Contains a structure with two fields: `scale` of type `INT32` that contains the scale of the transferred value and `value` of type `BYTES` containing the original value in an unscaled form.

When the `decimal.handling.mode` property is set to `double`, the connector represents `NUMBER` values as Java `double` values with schema type `FLOAT64`.

When the `decimal.handling.mode` property is set to `string`, the connector represents `NUMBER` values as their formatted string representation with schema type `STRING`.

|`NUMBER(P, S \<= 0)`
|`INT8` / `INT16` / `INT32` / `INT64`
|`NUMBER` columns with a scale of 0 represent integer numbers.
A negative scale indicates rounding in Oracle, for example, a scale of -2 causes rounding to hundreds. +
 +
Depending on the precision and scale, one of the following matching Kafka Connect integer type is chosen: +

 * P - S < 3, `INT8` +
 * P - S < 5, `INT16` +
 * P - S < 10, `INT32` +
 * P - S < 19, `INT64` +
 * P - S >= 19, `BYTES` (`org.apache.kafka.connect.data.Decimal`)

When the `decimal.handling.mode` property is set to `double`, the connector represents `NUMBER` values as Java `double` values with schema type `FLOAT64`.

When the `decimal.handling.mode` property is set to `string`, the connector represents `NUMBER` values as their formatted string representation with schema type `STRING`.


|`NUMBER(P, S > 0)`
|`BYTES`
|`org.apache.kafka.connect.data.Decimal`

|`NUMERIC[(P, S)]`
|`BYTES` / `INT8` / `INT16` / `INT32` / `INT64`
|`org.apache.kafka.connect.data.Decimal` if using `BYTES` +
 +
Handled equivalently to `NUMBER` (note that S defaults to 0 for `NUMERIC`).

When the `decimal.handling.mode` property is set to `double`, the connector represents `NUMERIC` values as Java `double` values with schema type `FLOAT64`.

When the `decimal.handling.mode` property is set to `string`, the connector represents `NUMERIC` values as their formatted string representation with schema type `STRING`.

|`SMALLINT`
|`BYTES`
|`org.apache.kafka.connect.data.Decimal` +
 +
`SMALLINT` is mapped in Oracle to NUMBER(38,0) and hence can hold values larger than any of the `INT` types could store

|`REAL`
|`STRUCT`
|`io.debezium.data.VariableScaleDecimal` +
 +
Contains a structure with two fields: `scale` of type `INT32` that contains the scale of the transferred value and `value` of type `BYTES` containing the original value in an unscaled form.

When the `decimal.handling.mode` property is set to `double`, the connector represents `REAL` values as Java `double` values with schema type `FLOAT64`.

When the `decimal.handling.mode` property is set to `string`, the connector represents `REAL` values as their formatted string representation with schema type `STRING`.

|===

[id="oracle-boolean-types"]
.Boolean types

Oracle does not provide native support for a `BOOLEAN` data type.
However, it is common practice to use other data types with certain semantics to simulate the concept of a logical `BOOLEAN` data type.

To enable you to convert source columns to Boolean data types, {prodname} provides a `NumberOneToBooleanConverter` {link-prefix}:{link-custom-converters}#custom-converters[custom converter] that you can use in one of the following ways:

* Map all `NUMBER(1)` columns to a `BOOLEAN` type.
* Enumerate a subset of columns by using a comma-separated list of regular expressions. +
To use this type of conversion, you must set the xref:oracle-property-converters[`converters`] configuration property with the `selector` parameter, as shown in the following example:
+
[source]
----
converters=boolean
boolean.type=io.debezium.connector.oracle.converters.NumberOneToBooleanConverter
boolean.selector=.*MYTABLE.FLAG,.*.IS_ARCHIVED
----

[id="oracle-temporal-types"]
.Temporal types

Other than the Oracle `INTERVAL`, `TIMESTAMP WITH TIME ZONE`, and `TIMESTAMP WITH LOCAL TIME ZONE` data types, the way that the connector converts temporal types depends on the value of the `time.precision.mode` configuration property.

When the `time.precision.mode` configuration property is set to `adaptive` (the default), then the connector determines the literal and semantic type for the temporal types based on the column's data type definition so that events _exactly_ represent the values in the database:

[cols="25%a,20%a,55%a",options="header"]
|===
|Oracle data type |Literal type (schema type) |Semantic type (schema name) and Notes

|`DATE`
|`INT64`
|`io.debezium.time.Timestamp` +
 +
Represents the number of milliseconds since the UNIX epoch, and does not include timezone information.

|`INTERVAL DAY[(M)] TO SECOND`
|`FLOAT64`
|`io.debezium.time.MicroDuration` +
 +
The number of micro seconds for a time interval using the `365.25 / 12.0` formula for days per month average. +
 +
`io.debezium.time.Interval` (when `interval.handling.mode` is set to `string`) +
 +
The string representation of the interval value that follows the pattern `P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S`, for example,  `P1Y2M3DT4H5M6.78S`.

|`INTERVAL YEAR[(M)] TO MONTH`
|`FLOAT64`
|`io.debezium.time.MicroDuration` +
 +
The number of micro seconds for a time interval using the `365.25 / 12.0` formula for days per month average. +
 +
`io.debezium.time.Interval` (when `interval.handling.mode` is set to `string`) +
 +
The string representation of the interval value that follows the pattern `P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S`, for example,  `P1Y2M3DT4H5M6.78S`.

|`TIMESTAMP(0 - 3)`
|`INT64`
|`io.debezium.time.Timestamp` +
 +
Represents the number of milliseconds since the UNIX epoch, and does not include timezone information.

|`TIMESTAMP, TIMESTAMP(4 - 6)`
|`INT64`
|`io.debezium.time.MicroTimestamp` +
 +
Represents the number of microseconds since the UNIX epoch, and does not include timezone information.

|`TIMESTAMP(7 - 9)`
|`INT64`
|`io.debezium.time.NanoTimestamp` +
 +
Represents the number of nanoseconds since the UNIX epoch, and does not include timezone information.

|`TIMESTAMP WITH TIME ZONE`
|`STRING`
|`io.debezium.time.ZonedTimestamp` +
 +
A string representation of a timestamp with timezone information.

|`TIMESTAMP WITH LOCAL TIME ZONE`
|`STRING`
|`io.debezium.time.ZonedTimestamp` +
 +
A string representation of a timestamp in UTC.

|===

When the `time.precision.mode` configuration property is set to `connect`, then the connector uses the predefined Kafka Connect logical types.
This can be useful when consumers only know about the built-in Kafka Connect logical types and are unable to handle variable-precision time values.
Because the level of precision that Oracle supports exceeds the level that the logical types in Kafka Connect support, if you set `time.precision.mode` to `connect`, *a loss of precision* results when the _fractional second precision_ value of a database column is greater than 3:

[cols="25%a,20%a,55%a",options="header"]
|===
|Oracle data type |Literal type (schema type) |Semantic type (schema name) and Notes

|`DATE`
|`INT32`
|`org.apache.kafka.connect.data.Date` +
 +
Represents the number of days since the UNIX epoch.

|`INTERVAL DAY[(M)] TO SECOND`
|`FLOAT64`
|`io.debezium.time.MicroDuration` +
 +
The number of micro seconds for a time interval using the `365.25 / 12.0` formula for days per month average. +
 +
`io.debezium.time.Interval` (when `interval.handling.mode` is set to `string`) +
 +
The string representation of the interval value that follows the pattern `P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S`, for example,  `P1Y2M3DT4H5M6.78S`.

|`INTERVAL YEAR[(M)] TO MONTH`
|`FLOAT64`
|`io.debezium.time.MicroDuration` +
 +
The number of micro seconds for a time interval using the `365.25 / 12.0` formula for days per month average. +
 +
`io.debezium.time.Interval` (when `interval.handling.mode` is set to `string`) +
 +
The string representation of the interval value that follows the pattern `P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S`, for example,  `P1Y2M3DT4H5M6.78S`.

|`TIMESTAMP(0 - 3)`
|`INT64`
|`org.apache.kafka.connect.data.Timestamp` +
 +
Represents the number of milliseconds since the UNIX epoch, and does not include timezone information.

|`TIMESTAMP(4 - 6)`
|`INT64`
|`org.apache.kafka.connect.data.Timestamp` +
 +
Represents the number of milliseconds since the UNIX epoch, and does not include timezone information.

|`TIMESTAMP(7 - 9)`
|`INT64`
|`org.apache.kafka.connect.data.Timestamp` +
 +
Represents the number of milliseconds since the UNIX epoch, and does not include timezone information.

|`TIMESTAMP WITH TIME ZONE`
|`STRING`
|`io.debezium.time.ZonedTimestamp` +
 +
A string representation of a timestamp with timezone information.

|`TIMESTAMP WITH LOCAL TIME ZONE`
|`STRING`
|`io.debezium.time.ZonedTimestamp` +
 +
A string representation of a timestamp in UTC.

|===

[id="oracle-rowid-types"]
.ROWID types

The following table describes how the connector maps ROWID (row address) data types.

.Mappings for Oracle ROWID data types
[cols="20%a,15%a,55%a",options="header"]
|===
|Oracle Data Type
|Literal type (schema type)
|Semantic type (schema name) and Notes

|`ROWID`
|`STRING`
|
n/a

|`UROWID`
|n/a
|_This data type is not supported_.

|===

[id="oracle-user-defined-types"]
.User-defined types

Oracle enables you to define custom data types to provide flexibility when the built-in data types do not satisfy your requirements.
There are a several user-defined types such as Object types, REF data types, Varrays, and Nested Tables.
At this time, you cannot use the {prodname} Oracle connector with any of these user-defined types.

[id="oracle-supplied-types"]
.Oracle-supplied types

Oracle provides SQL-based interfaces that you can use to define new types when the built-in or ANSI-supported types are insufficient.
Oracle offers several commonly used data types to serve a broad array of purposes such as *Any*, *XML*, or *Spatial* types.
At this time, you cannot use the {prodname} Oracle connector with any of these data types.

[[oracle-default-values]]
.Default Values
If a default value is specified for a column in the database schema, the Oracle connector will attempt to propagate this value to the schema of the corresponding Kafka record field.
Most common data types are supported, including:

* Character types (`CHAR`, `NCHAR`, `VARCHAR`, `VARCHAR2`, `NVARCHAR`, `NVARCHAR2`)
* Numeric types (`INTEGER`, `NUMERIC`, etc.)
* Temporal types (`DATE`, `TIMESTAMP`, `INTERVAL`, etc.)

If a temporal type uses a function call such as `TO_TIMESTAMP` or `TO_DATE` to represent the default value, the connector will resolve the default value by making an additional database call to evaluate the function.
For example, if a `DATE` column is defined with the default value of `TO_DATE('2021-01-02', 'YYYY-MM-DD')`, the column's default value will be the number of days since the UNIX epoch for that date or `18629` in this case.

If a temporal type uses the `SYSDATE` constant to represent the default value, the connector will resolve this based on whether the column is defined as `NOT NULL` or `NULL`.
If the column is nullable, no default value will be set; however, if the column isn't nullable then the default value will be resolved as either `0` (for `DATE` or `TIMESTAMP(n)` data types) or `1970-01-01T00:00:00Z` (for `TIMESTAMP WITH TIME ZONE` or `TIMESTAMP WITH LOCAL TIME ZONE` data types).
The default value type will be numeric except if the column is a `TIMESTAMP WITH TIME ZONE` or `TIMESTAMP WITH LOCAL TIME ZONE` in which case its emitted as a string.

